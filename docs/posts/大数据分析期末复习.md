---
title: 大数据分析期末复习
date: 2023-01-25 16:41:08
tags:
- 期末复习
- 大数据分析
---

# 复习大纲

**适当关注，不太全，有什么能帮到你的可以在右下角联系**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301291701142.jpeg)

# 关联规则

## 置信度、支持度、强关联规则

**置信度**

- 置信度表示的一个两个事物之间的关系

例子

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301252049591.png)

$beer \Rightarrow Diaper = 3 / 3 = 100%$,表明了买beer的人一定同时也会买Diaper

**关联规则和强关联规则**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301252054400.png)

**要注意的是强关联规则的边界是大于（含等于）上图有误，可以参照下面例题二）**

- **支持度**提示了事物X和Y**同时**出现的概率; 
- 置信度提示了**X出现时**， Y出现的概率。

**例题**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301252057335.png)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301252104270.jpeg)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301252105344.png)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301252107673.jpeg)

**例题2**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301252109335.png)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301252135278.jpeg)

## Apriori算法

Apriori主要讲的是两个事

- 如果某个项集是频繁集，那么他的所有非空子集一定也是频繁项集。
- 如果某个项集非频繁项集，那么其所有的超集都是非频繁项集

**例子**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301252141336.png)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301252141964.png)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301252141900.png)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301252142909.png)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301252142645.png)

这里的 $A => BC [support = \frac{2}{8},confident = \frac{2}{6}]$

**例题一**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301252142817.png)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302121511135.jpeg)

##  AprioriTid 算法

**例子**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301252209334.png)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301252210057.png)



![截屏2023-01-25 22.10.57](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301252211934.png)

**例题一**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301252211244.png)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301252215637.jpeg)

因为这里给的例题比较简单，所以建议手写一下例子

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301291658701.jpeg)

## FP-tree

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301292136760.png)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301292232343.jpeg)

**例题二**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301292232525.png)



# 分类算法

## 决策树

### ID3（**信息熵**）

**信息熵**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302011628002.png)

**信息增益**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302011630369.png)

**例子**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302011634185.png)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302011634497.png)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302011634046.png)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302011635597.png)

**例子**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302012016711.png)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302012017652.jpeg)

### CART 算法（基尼指数）

**基尼指数**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302012018576.png)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302012021162.png)

![ ](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302012021884.png)

**过拟合与欠拟合**

**过拟合：**机器的学习能力过于强大，把一些训练样本的所包含的不太一般的特性都学到。过拟合是各类学习算法的一个关键屏障，一般来说，我们需要通过对多个候选模型的泛化误差评估，选择泛化误差最小的模型。

**欠拟合：**相反，是机器学习能力过弱导致的，没有讲训练样本的包含的一般特性训练到。比较容易克服，在决策树当中可以通过拓展分支来克服，在神经网络当中可以通过增加训练的轮数来克服。

## 线性分类器

# 模型评估与选择

## 选择训练集

### 留一法

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302031814144.png)

### **k折交叉验证法**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302031816353.png)

**也就是把样本随机分成K份，选用不同划分p次。就后就是p次k折交叉验证**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302031817240.png)

### 自助法

有放回的取样，主要用于数据样本比较少的情况，不容易划分测试集和训练集

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302031818439.png)

## 性能度量

### PR图

回归问题常用**均方误差**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302031819732.png)

**查准率**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302031821048.png)

预测正类在所有预测正类中的比例

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302031821202.png)

**查全率**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302031822835.png)

PR算法步骤

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302032036810.png)

1. 一定不要忘记对预测结果进行**排序**
2. 计算查全率(P)和查准率(R)
3. 最后根据BEP平衡点或者F1度量找出来最优的点

**F1度量**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302032101148.png)

**宏（微）查准率/查全率**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302032102282.png)

**例子**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302032110857.png)

### ROC，AUC图

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302032103137.png)

**TPR = $\frac{TP}{真实的正例个数}$**

**FPR = $\frac{FP}{真实反例个数}$**

**例子**

![截屏2023-02-03 21.09.53](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302032109580.png)

# 聚类模型

**距离公式**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302082059270.jpeg)

## K-mean

![截屏2023-02-08 21.00.48](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302082100300.png)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302082101827.png)

**简单来说就是：**

1. 初始化时候，按照要分多少个类取多少个随机点
2. 按照某一个距离公式，计算每个点到我们的聚类中心的距离
3. 把距离聚类中心最近的点分为同一个类别
4. 对比聚类分类结果是否有变化，如果没有变化的话就结束聚类
5. 聚类分类结果变化了就需要通过取同一个类中的样本平均值，更新聚类中心

**练习**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302082105900.png)

PAM

**练习**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302091958972.jpg)

# 最后一道开放题（建议看看）

## 数据清洗与预处理

**首先导入数据**

使用pandas导入数据

**缺失值的处理**

1. 对于相对少量的缺失值可以直接删除
2. 对于年龄这种参数可以直接使用平均值来替代（使用sklearn中的imputer类）
3. 如果使用了决策树这类模型还可以给每一个缺失值加上一个权重

**展开来说将缺失值替换成均值**

对于普通的连续或者离散型的数据都是比较好取均值的，但是如果类别是猫、狗和麋鹿这种怎么取均值呢？创建哑变量

可以为了猫狗都创建一个变量0/1.如果原始列为猫的时候，那么对应的狗和麋鹿都为0，猫则是1。

**训练集和测试集的划分**

使用K-折交叉验证

- 保持数据的一致性（分层采样，即需要样例比例测试集和训练集相同）
- 多次重复划分
- 测试集不能太大也不能太小

如果样本点实在太小的话自助法，有放回的取样

**特征缩放**

因为一个事物有多个属性，不同的属性可能对应的量纲不同，所以我们需要进行一个标准化处理的过程，在这里也就是特征的缩放。`StandarScaler()`函数是我们经常调用的。

**标准化的过程是将每个维度的特征数据方差为1，均值为0，这样的预测结果不会由于某些维度数值过大的特征值而主导**

## 数据转换

假设这里要转换的数据类型为日期类型那么就需要进行数据的转换，提取特征的相关特性，进行映射到一个量纲上的数值。（很多东西也可以同理类比）

## 特征选取

特征选择主要有两个功能：

- 减少特征数量、降维，使模型泛化能力更强，减少过拟合
- 增强对特征和特征值之间的理解

主要的方法有以下

1. 去除变化率比较小的变量

   假设某个特征值的只有0/1的时候，95%情况下的样本都是出现的是1，那么这个特征的就没有意义，所以要把当前的特征给去除。值得注意的是在连续是需要做一步连续值离散化在去作去比较。

2. 单变量特征选择

   1. 皮尔森相关系数

      使用皮尔森相关系数，了解变量之间的线性关系，取值为[-1,1],-1表示的是变量负相关，0表示没有关系，1表示变量正相关。这里就需要剔除与我们需要预测的变量结果负相关的甚至相关度不的特征。

      需要值得注意的是皮尔森相关系数只能预测变量的线性关系。

   2. 距离相关系数

      使用距离相关系数克服了变量之间的非线性关系

3. 线性模型和正则

   在很多的机器学习的算法中都伴随着特征的打分机制，比如在回归模型中的越重要的特征在模型中的权重也就会越高，相关特征相关度不高的特征在特征的打分机制就会越加的趋近于0。

   1. 正则化模型

      也就是说把额外的约束或着惩罚加到已有的模型上面，**以防止过拟合并且特高泛化能力**

   2. **L1正则化**

      L1正则化将系数w的l1范数作为惩罚加到损失函数上，由于正则项非零，这就迫使那些弱的特征所对应的系数变成0。因此L1正则化往往会使学到的模型很稀疏（系数w经常为0）

4. 随机森林

   1. 随机取样训练决策树
   2. 随机选取属性作节点分裂属性
   3. 重复2，直到节点不能分裂
   4. 建立大量的决策树，构成随机森林

## 模型建立

### 分类问题

#### 逻辑回归

分类问题使用逻辑回归

逻辑回归就是在线性模型层面上多添加的一层映射

$h_{\theta(x) = g(\theta^Tx) = \frac{1}{1+e^-\theta^T x}}$

$g(z)=\frac{1}{1+e^{-z}}$

这样就可以使用0.5作为一个一般化的阈值进行分类处理了。

同样的参数的调整依然可以使用梯度下降法。

#### 感知器算法

引入多隐层进行非线性的分类判别

原理

不断给出样本进行迭代参数处理。参数的调整主要是用了BP误差逆传播算法，将输出结果的误差平摊到每一个权重上面。

### 回归问题

### 线性回归

有些只考虑到了回归的线性关系，采用梯度下降或最小二乘法进行参数的调节。

MAE,MSE, RMSE,NRMSE

### 多项式回归

在最后评估的时候还可以使用 $R^2$指标进行评价

R2 是多元回归中的回归平方和占总平方和的比例,它是度量多元回归方程中拟合程度的一个统计量,反映了在因变量y的变差中被估计的回归方程所解释的比例。
R2 越接近1,表明回归平方和占总平方和的比例越大,回归线与各观测点越接近,用x的变化来解释y值变差的部分就越多,回归的拟合程度就越好。

## 评估模型

**一、分类**

> **1、混淆矩阵**
> **2、准确率（Accuracy）**
> **3、错误率（Error rate）**
> **4、精确率（Precision）**
> 5、召回率（Recall）
> 6、F1 score
> 7、ROC曲线
> 8、AUC
> **9、PR曲线**
> 10、对数损失（log_loss）
> 11、分类指标的文本报告（classification_report）

**二、回归**

> 1、平均绝对误差（MAE）
> 2、均方误差（MSE）
> 3、均方根误差（RMSE）
> 4、归一化均方根误差（NRMSE）
> 5、决定系数（R2）

## 进一步提高准确率

采用集成学习
