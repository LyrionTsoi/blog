---
title: 数据分析方法实验五
date: 2023-06-02 10:56:02
tags:
---

## 三. 实验过程

#### 对于公式推导的理解

贝叶斯线性回归的目的是找到线性模型中的权重**w**的后验概率分布，即在给定数据的情况下，**w**有多大的可能性取不同的值。为了求解这个分布，我们需要用到贝叶斯定理，它告诉我们如何根据先验概率和似然概率来更新后验概率。先验概率表示我们在观察数据之前对**w**的信念，似然概率表示数据在给定**w**的情况下的可能性，后验概率表示我们在观察数据之后对**w**的信念。贝叶斯定理可以写成如下的公式：

$$p(w|Data) = \frac{p(Data|w)p(w)}{p(Data)}$$

其中，$p(w|Data)$是后验概率，$p(Data|w)$是似然概率，$p(w)$是先验概率，$p(Data)$是数据的边缘概率，它相当于一个归一化常数，可以通过对所有可能的**w**进行积分来得到。

为了简化问题，我们假设先验概率是一个高斯分布，即$w \sim N(0, \Sigma_p)$，其中$\Sigma_p$是一个协方差矩阵，表示**w**的不确定性。我们也假设似然概率是一个高斯分布，即$y \sim N(w^Tx, \sigma^2)$，其中$\sigma^2$是一个方差，表示数据的噪声。由于高斯分布的共轭性，后验概率也是一个高斯分布，即$w|Data \sim N(\mu_w, \Sigma_w)$，其中$\mu_w$和$\Sigma_w$是后验分布的均值和协方差。

为了求解$\mu_w$和$\Sigma_w$，我们需要将似然概率和先验概率代入贝叶斯定理，并对比高斯分布的一次项和二次项。具体的推导过程可以参考¹或²，这里只给出最终的结果：

$$\left\{\begin{aligned}
& \mu_w = \sigma^{-2}A^{-1}X^TY \\
& \Sigma_w = A^{-1} \\
& A = \sigma^{-2}X^TX + \Sigma_p^{-1}
\end{aligned}\right.$$

其中，$X$是一个包含所有自变量的矩阵，$Y$是一个包含所有因变量的向量。有了后验分布的均值和协方差，我们就可以计算任何新数据点的预测值$f(x^*)|Data, x^*$的概率分布，它也是一个高斯分布，其均值和方差为：

$$\left\{\begin{aligned}
& E[f(x^*)|Data, x^*] = \mu_w^Tx^* \\
& Var[f(x^*)|Data, x^*] = \sigma^2 + x^{*T}\Sigma_wx^*
\end{aligned}\right.$$

这样，我们就可以得到预测值的点估计和置信区间，从而反映出模型参数的不确定性。

#### 代码

```python
import numpy as np
import matplotlib.pyplot as plt


def generate():
    x = np.linspace(50, 150)
    w = 3.6
    y = w * x + np.random.normal(0, 30, size=x.size)
    X = x.reshape(-1,1)
    Y = y.reshape(-1,1)
    return X, Y

class LinearRegression(object):
    def __init__(self, w=None):
        self.w = w

    def least_square(self, X, Y):
        '''
        最小二乘法，通过给定样本学习参数
        X: 样本矩阵，一行一样本
        Y: 样本标签，Nx1矩阵
        '''
        inv = np.linalg.inv(np.dot(X.T, X))
        self.w = inv.dot(np.dot(X.T, Y))
        
    def ridge(self, X, Y, lam=3e-2):
        # 岭回归
        inv = np.linalg.inv(np.dot(X.T, X) + np.diag([lam]*X.shape[-1]))
        self.w = inv.dot(np.dot(X.T, Y))

    def predict(self, x):
        '''
        通过给定数据进行预测
        '''
        return np.dot(x, self.w)


class BayesRegression(object):
    def __init__(self, sigma, mu=None, cov=None):
        self.sigma = sigma  # 噪声方差
        self.mu = mu        # 后验分布的均值
        self.cov = cov      # 后验分布的协方差矩阵

    def fit(self, X, Y):
        '''
        拟合后验参数
        '''
        prior = np.eye(X.shape[-1])     # 先验协方差
        A = np.dot(X.T, X) / self.sigma
        self.cov = np.linalg.inv(A)
        self.mu = self.cov.dot(np.dot(X.T, Y)) / self.sigma

    def generate(self, x):
        '''
        根据给定数据，生成预测值
        '''
        mean = np.dot(x, self.mu).sum()
        std = np.sqrt(np.dot(x, self.cov).dot(x)).sum()
        return np.random.normal(mean, std, 1)

    def generate_random(self, x):
        '''
        根据给定数据，生成带噪声的预测值
        '''
        mean = np.dot(x, self.mu).sum()
        std = np.sqrt(np.dot(x, self.cov).dot(x) + self.sigma).sum()
        return np.random.normal(mean, std, 1)
        

def main():
    X, Y = generate()
    lr = LinearRegression()
    lr.ridge(X, Y)
    x = np.array([50, 150]).reshape(-1,1)
    y = lr.predict(x)

    plt.figure()
    plt.scatter(X, Y)
    plt.plot(x, y, c='r')
    for i, j in zip(X, Y):
        plt.plot([i,i], [j, lr.predict(i)], c='b', linestyle='dotted')
    plt.show()


if __name__ == "__main__":
    # main()
    X, Y = generate()
    lr = LinearRegression()
    lr.least_square(X, Y)
    sigma = np.square(Y - np.dot(X, lr.w)).mean()
    '''
    关于贝叶斯线性回归需要将噪声的方差传给模型，但是计算方差又需要具体的w值
    所以没办法只能通过判别模型的线性回归拟合w，然后再计算噪声的方差
    '''
    br = BayesRegression(sigma)
    br.fit(X, Y)
    
    xx = []
    yy = []
    count = 30
    for x in np.linspace(50, 150):
        xx += [x] * count
        yy += [br.generate_random(x) for _ in range(count)]
    
    plt.figure(figsize=(21,9))
    plt.scatter(xx, yy, c='r')
    plt.show()
```

运行结果

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202306021058330.png)



$a_{n+1} = a_n + 10^{-n}$
