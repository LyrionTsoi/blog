---
title: 人工智能期末复习
date: 2023-01-14 16:53:49
tags:
- 人工智能
- 期末复习
---

# 考试重点

## 计科04

老师原话

我们这个内容的话，大家拿到PPT的话会有很多，然后那么绪论和知识表示这一块儿，大家大概的过一下就可以了。

那么我们以第三章为主，第三章的话，这里有盲目搜索、启发式搜索、与或图启发式搜索以及博弈搜索，其中启发式搜索、与或图启发式搜索和博弈搜索是重点，那么大家这个在这方面复习的时候要重点看一下。

那么第四章中，那么很多问题的话，大家都要好好看一看，就是这一章都要好好看一下，其中我我们讲的内容，特别是讲的内容是可信度方法，主观贝叶斯，那么，基于新人测度的证据理论，大家先都过一下，那么，其中主观贝叶斯方法的话是重点啊，大家要去看一看，特别是相关的例题一定要重新做一下。

另外的话，第五章机器学习这一段，这一段的话是这样子就是说它的基本分类方法以及关键几个数学问题，大家一定要好好看一看，那么还有一个问题，大家可以思考一下，我在上课给大家讲过，就是说我们对于分类问题的设计的基本思想是什么？我们在PPT里面有给大家展现，那么大家要找一下看看。

还有梯度下降法的一些设计啊，然后那么几个关键数学问题，大家回去好好看一看啊。

那么线性分类器这一章的话就是五杠三，这一章，那么非学线性判比，大家要看一下它的相关的理论设计，那么感知计算法的话要好好看一看怎么做的啊？就是它的设计基本思想都是哪些，那么这里也会给大家在这一章的开头的话，也会给大家给出来一个分类器设计的三个基本要素是什么，大家可以看一下，这个要重点哦。

在非线性分类器设计这个PPT里面，SVM的设计思想，那么什么是SVM是什么？是知识向量，那么这个问题在上课里问过大家很多次了，那么这次啊到实战上了那么波情算法的设计思想，那么大家可以要去看一下。

那么，呃，我们讲了很多，这个神经网络啊，我说了人工神经网络这一章的话，那么它的这个更新方式，大家可以注意一下，那么啊，我们讲了几种神经网络啊，包括CN啊，然后还有这个自组织how的贤馈，然后还有最啊，简单的这个多层感知器。

那么深度学习这一章的话就是CN，那么，CN的那个计算什么，就是我这一层，有多少个这个神经元啊，我这个呃，多少个？这个这个，我这啊，非常马的大小多少？哎，大家还记得吗？有一页PPT专门展现的这个。那么这些PPT的话，大家可以应该很熟啊啊，那么就是卷积的相关计算啊，就大家可以找一下，那么这一块儿的话是重点。

当然了，CN的设计思想，大家也要看一下，然后CN的这些这个后面的两个使用技巧，一个是如何提升你的网络训练方训练效果，另外一个是这个这个CN设计的基本原理啊。

那我如何优化神经网络的优化策略和作用？大家可以再看一下啊，那么比如说我用drop奥的技术是怎么样子的？那么这个是嗯，总体概括，那么我再接下来说几个重点。

我们说重点的话，首先是多分类和二分类的问题啊，我们在讲分类题的时候又给大家以二分类为基础，往下讲的那么多分类问题的解决，所以呃，那几种方法大家在学完感知器以后会学了一个多分类，那么一对一模式的设计，多分类分类器，那么怎么来去做哪个属于哪一类啊？大家注意一下。

我如何用梯度下降法去更新模型，这个事情大家要注意一下，我们在这个讲这个先进回归的时候给大家呃多次训练过啊，特别是踢球太量法的这个呃计算方式，那么，踢球天想法的本身的一些问题，重要的一些它的一些优缺点啊，这些大家要注意一下。

最后那么强调一下主观VS的例题啊，大家一定要回去看一下。

## 图灵

**题型：简答，计算，综合分析题**

### 第三章 搜索

**爬山算法**

- 基本思想
- 缺点是什么

**最好优先算法**

- 好处，思想，相对于爬山算法哪里会比较好

**八数码问题**

- 两种启发式函数
- 利用最好优先算法求解八数码问题的步骤，能画出搜索图，open & close

**通用图算法**

- 了解是什么即可

**A星**

- 启发式信息的强弱对搜索的影响是什么
- 要会画搜索图

**博弈术搜索**

- 研究博弈树搜索是哪两个方面
- 极大极小(基本思想，评估函数值，缺点)
- $\alpha-\beta剪枝$（基本过程，缺点）

### 第四章 不确定性推理

**可信度方法**

- 例题
- ![可信度方法](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302141636264.png)

**主观贝叶斯**

- EH公式和CP公式不用记，用到会题目会给出
- 难度不会超过综合练习题-2

### 第五章 机器学习

#### 5-1

- ![机器学习的一般思路](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302141647704.png)
- 回归问题和分类问题的值域范围
- 需要知道最小二乘法的局域性，其他的不需要会
- 🌟梯度下降（基本步骤，对于最基本的损失函数要知道如何求解，影响因素产生什么样的结果，没有提到BGD不知道要不要考？？）
- 逻辑回归和线性回归的区别

#### 5-3线性分类器

- 线性分类器的三个基本要素
- Fisher线性判别（基本思想）
- 多类问题（有一对多等这些方式，要掌握每一种方式是如何做的，问题是什么；
- 🌟感知器的单样本修正法，要会写出基本步骤，例题（多类的不要求）

#### 5-4非线性分类器

**SVM**

- 什么是最大间隔
- 三个方法软/硬间隔以及核方法和主要是用于哪里的
- 支持向量是什么
- 软间隔和硬间隔的区别
- 核方法的作用是什么
- 里面的例子给出来 $\alpha$要知道支持向量是如何求解，判别函数如何写

**adaboost**

- 🌟强分类器的设计思想、基本思路
- 级联分类器的设计思想

### 第六章 人工神经网络

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302141710201.png)

- BP算法的基本思想
- 径向基函数（只需要了解下图）
- ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302141714629.png)
- RBF与BP神经网络的区别

### 第七章 深度学习

#### 7-1卷积神经网络

- BP算法为什么不能用于深度学习里面
- 卷积神经网络的核心思想（局部感知，权值共享，多卷积核，空间下采样）
- 卷积神经网络的神经元个数、参数（权值）个数 LeNet-5

#### 7-2 CNN

- Relu函数主要的优缺点
- 为什么要正则化，正则化的作用
- Dropout基本思路、目标、与正则化的区别
- 批规范化的优化策略

#### 7-4集成学习

- ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302141723941.png)

# 第三章 搜索

## 启发式搜索

### 局部择优搜索(爬山算法)

贪心思想

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202301300019150.jpg)

### 全局择优搜索(有序搜索/最好优先)

# 不确定性推理

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302021552374.png)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302021616728.jpeg)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302021616755.jpeg)

# 机器学习基础

**估计函数**

**二维的情况**

$h_\theta(x) = \theta_1x + \theta_0$

**三维的情况**

$h_\theta(x)=\theta_1x_1 + \theta_2x_2 + \theta_0$

可以统一的形式为

$h_\theta(x) = \theta^TX$

**损失函数**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302022040394.png)

也就是预测值与实际值的平方除上个数

为了找出最好权重，**最小化**损失函数

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302022041711.png)

这里的 $\frac{1}{2}$是为了方便函数求导使用的

**调整使用 $\theta$使得 $J(\theta)$取得最小值的方法需要重点注意**

## **最小二乘法**

在二位的情况下有下面的回归模型

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302022051671.png)**克莱姆法则**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302022052085.png)

**最小二乘法-矩阵表达形式**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302022053421.png)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302022054222.png)

矩阵的形式公式： $\beta = (A^TA)^{-1}A^TY$

## 梯度下降法

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302022108513.jpeg)

- 梯度下降**不一定能够找到全局的最优解**，有可能是一 个局部最优解。当然，如果损失函数是**凸函数**，梯度下 降法得到的解就**一定是**全局最优解。
- 步长太大，会导致迭代过快，甚至有可能错过最优解 。步长太小，迭代速度太慢，很长时间算法都不能结束 。所以算法的步长需要多次运行后才能得到一个较为优的值。

**SGD随机梯度下降法**

![、](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302022239060.png)

**BGD**

使用所有已有的样本数据

**MBGD**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302022240920.png)

例题

**最小二乘法**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302022132504.jpeg)



![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302022235598.jpeg)

# 线性分类器

**三个基本要素:**

1. 判别函数的类型:从什么样的函数类中去求解
2. 分类器设计的目标/准则，通常就是确定函数类中的某些待定参数 
3. 如何设计算法利用样本数据搜索到最优的函数参数

## 感知器

**感知器准则函数**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302112107865.png)

**单样本修正**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302112109451.png)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302112110241.jpeg)

感知器判别函数

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302112110021.png)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302112111138.jpeg)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302112111526.png)

# 简答题

1. 监督学习和非监督学习的区别

   **监督学习：**是根据**已知类别**的样本，由机器从中学习并且训练，从中勾画出出各类事物在**特征空间分布的规律性**，从而对新样本进行判断

   **无监督学习/聚类：**由机器从未知样本中进行自学习，从中发现有利于对象分类的规律

   **了解一下什么是半监督学习：**由机器利用**部分已知**类别的样本，从中恢复样本的相关信息，进而进行聚类分析

   ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302021638040.png)

2. 机器学习的一般思路，画出框图来，每一个方框能干什么简短述

   一般思路:
    从开始的通过传感器(例如 CMOS)来获得数据。然后经过预处理、特征提取、特征选择，再到推理、预测或者识别。

   获取原始数据样本集，然后进行特征提取来减少特征量，简化后序步骤的运算量，得到对应的特征后选择分类器进行训练，训练完一个分类器，新来一个样本的时候可用已经训练好的分类器进行预测。

## 支持向量机

1. 什么是支持向量机？

   一种对**线性和非线性**的分类方法。通过向量内积的回旋，**引入非线性核函数将问题变为高维特征空间与低维输入空间的相互转换**，解决了数据挖掘中的维数灾难。由于计算问题转换成凸二次规划问题，因此挖掘算法是误解或有全局最优解。

2. 什么是支持向量？

   简单来说，就是支持或支撑平面上把两类类别划分开的向量点。向量点正好在间隔边界上。

   **从几何意义来说：**超平面法向量是支持向量的线性组合

3. 支持向量机的目标？

   找到一个超平面，使得能尽可能多的将两类数据明确的分开，同时使得分开的两类数据点距离分类面最远

4. 什么是最大几何间隔？

   两类样本中离超平面最近的样本与超平面之间的距离是最大的

5. 最优分类超平面？

   一个超平面，能够将训练样本没有错误的分开，并且两类训练样本中离超平面最近的样本与超平面之间距离最大。

6. 硬间隔与软间隔的区别？

   **硬间隔：**线性可支持向量机：分类完全正确，不存在损失函数，损失量为0

   **软间隔：**同样是线性可支持向量机，但是在某一些具有噪声影响的模型里面，使用硬间隔无法完全将类别分类，有个别的 **离群点**。那么使用软间隔就是**允许样本有一定程度的错分**，实质依然是线性分类器

7. 核函数的作业

   将低维特征映射到高维的特征空间，使得样本变成了线性可分的情况。使映射不必显式地计算，同时对映射特征的计算依然很高效。

8. 核函数的选择

   ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302071727945.png)

9. **SVM**实现非线性分类的思想

   1. 给点训练集后，决策函数依赖于 $K(x_i,x_j) = ((x_i * x_j) + 1)^2$
   2. 如果想用其他的非线性分划办法，则可以考虑其他形式的函数 $k(x_i,x_j)$

## 感知器算法

1. 不同隐层情况下的神经网络

   ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302112008789.png)

   简单来说就是：无隐层对应的判决域为半平面，单隐层凸域，双隐层任意复杂域

    

## 🌟Boosting算法(Adaboost分类器)

1. Boosting算法思想

   通过把若干个弱分类器结合到一个强分类器中，从而提高分类器性能的方法。强分类器对数据进行分类，是通过弱分类器的多数投票而来的。

2. Adaboost算法原理

   1. AdaBoost算法是一种**迭代**的算法，对于一组训练集，通过**改变其中每个样本的分布概率**，而得到不同的训练集Si，对于每一个$S_i$进行训练从而得到一个**弱分类器**$H_i$， 再将这些**若分类器根据不同的权值组合**起来，就得到了强分类器。
   2. 初始的时候，每个样本都是均匀分布，通过训练得到分类器H0，在该训练集中，**分类正确**，就降低其分布概率;**分类错误**，就提高其分布概率，这样得到的新的训练集S1就主要是**针对不太好分类的样本**了。再使用S1进行训练，得到分类器H1， 依次迭代下去......，设迭代次数为T，则得到T个分类器。
   3. 对于每个分类器的权值，其分类**准确性越高**，**权值越高**

3. 弱分类器

   1. 二分类问题，只要求分类结果稍微高于50%
   2. 一个弱分类器有一个特征，一个判决阈值和一个控制参数组成
   3. 如何选择阈值
      1. 通过积分图快速计算所有训练样本对应某特征值
      2. 将特征值进行排序
      3. 选择分类错误率最小的特征值为判决阈值

4. 强分类器思想

   算法的训练过程，其实是特征的**“贪婪”**选取过程 

   基本的过程：

   1. **选择对正负样本区分最好的T个矩形特征，并训练成对应的弱分类器，把所有弱分类器组合成高性能的强分类器。**
   2. **在训练每个弱分类器时，对正负样本给予不同的权重。**在每一轮分类器的训练过程中，针 对上一轮中没有被正确分类的样本，下一轮的分类器就会更加“关注”它们。每一轮分类 器的循环训练是靠样本权值的更新来实现。权值改变的作用是在被误分的样本上设置更大 的权值，在分类正确的样本上减小其权值，凸显分类错误的样本，使其得到重视
   3. **通过训练的T个弱分类器的加权投票建立最终的强分类器**

   ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302072035065.png)

5. 描述 Adaboost 级联分类器的基本设计思路

   **思想：**联合多个强分类器，对非人脸采取“先重后轻”，快速、高效的检测出人脸。要求各级强分类器从简单到复杂，从而有效、快速的排除最多的非人脸 先重后轻:级联分类器的前几级构造小型、高效的强分类器用来滤除大量负样本，使待检测 的窗口越来越少，再调用后面相对复杂的强分类器，达到较低的误检率
    为了解决什么:
   1.Adaboost 算法存在退化现象，随着弱分类器个数的增加，强分类器的分类能力反而会降低， 需要改进
   2.Adaboost 在检测图像过程中要对每一个位置的每一种尺寸的窗口进行检测，检测量极大 3.在实际图片中，人脸区域只占整张图片的较小比例，Adaboost 在检测时，会花大量的时间在这些无人脸区域上

## 资料

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302092152675.jpg)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302092152696.jpg)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302092152713.jpg)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302092152734.jpg)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302092152761.jpg)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302092152747.jpg)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302092152209.jpg)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302092152227.jpg)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302092152245.jpg)

# 总习题练习

![习题练习-2](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302112000651.jpg)

![习题练习-3](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302112000671.jpg)

![习题练习-4](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302112000626.jpg)

![习题练习-5](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302112000590.jpg)

![习题练习-6](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302112000613.jpg)

![习题练习-7](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302112000597.jpg)

![习题练习-8](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302112000698.jpg)

![习题练习-9](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302112000707.jpg)

# 总复习（具体看这个）

## 第三章 搜索

**爬山算法**

- 基本思想

  利用评估函数 $f(x)$来估计目标状态和当前状态的距离

  1. 当一个节点被拓展后，对子节点x进行评估得到 $f(x)$,按 $f(x)$的升序排列并把这些节点压入栈。因此，栈顶元素具有最小的 $f(x)$值（贪心）。
  2. 弹出栈顶元素并和目标状态比较。如果栈顶元素不是目标，就继续拓展栈顶元素，并计算其所有子状态的f值，并按升序把这些子节点压入栈中
  3. 如果栈顶元素为目标节点的，算法退出。否则一直循环下去，直至栈为空。

- 缺点是什么？

  1. 局部最大
  2. 高地：也称为平顶，在某一局部点周围f(x)为常量。 此时，搜索就无法确定要搜索的最佳方向，会产生随机走动，这使得搜索效率降低。
  3. 山脊：山脊可能具有陡峭的斜面，所以搜索可以比较容易到达山脊的顶部，但是如果山脊的顶部到山峰之间可能倾斜的很平缓。除非正好有合适的操作直接沿着山脊的顶部移动，否则搜索可能就会在山脊的两面来回的震荡，搜索的前进步伐很小。

**最好优先算法**

- 好处，思想，相对于爬山算法哪里会比较好？

  1. 相对于爬山算法比较好的地方

     从最有希望的节点开始，并生成所有子节点，然后计算 **所有节点**的性能，基于性能选择最有希望的节点拓展，**不仅仅是从当前拓展的子节点中选择**。

  2. 搜索的过程

     1. 把初始节点放入open表当中，计算$f(s_0)$
     2. 如果open表为空的话，就表示算法无解，退出
     3. 把open表的第一节点(n)取出，放入到close表当中
     4. 考察节点n是否为目标节点，如果是的话，则求解到了问题的解，退出
     5. 若节点n不可拓展，则转到第二步
     6. 拓展n节点，用估价函数计算每一个子节点的估计值，并为每一个子节点都配置一个指向父节点的指针，把这些子节点都送入open表，然后对open表中全部节点按照估价值从小到大的顺序进行排序
     7. 转到第二步

**八数码问题**

1. 两种启发式函数

   1. $h(x)$“不在位”的将牌数
   2. $p(x)$ 将牌“不在位”的距离和，节点n的每一个数码与其目标位置间的距离总和（曼哈顿距离）

   很明显看出 $h(x)≤p(x)≤实际需要移动的步数$，其实预估函数越靠近实际步数就说明了启发信息就越好。

2. 利用最好优先算法求解八数码问题的步骤，能画出搜索图，open & close

**通用图算法**

- 了解是什么即可

**A星**

1. 启发式信息的强弱对搜索的影响是什么

   启发信息越强的话越能选出来的最优的搜索路径，启发信息越弱的话甚至最后退化成暴力搜索

   h(n)的选取大致有如下三种情况:

   - 如果h(n)< h*(n)到目标状态的实际距离，这种情况下，搜索的点数多 ，搜索范围大，效率低。但能得到最优解。

   - 如果h(n)= h*(n) ，即距离估计h(n)等于最短距离，那么搜索将严格 沿着最短路径进行， 此时的搜索效率是最高的。

   - 如果 h(n)> h*(n) ，搜索的点数少，搜索范围小，效率高，但不能保 证得到最优解。

2. 会画搜索图![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302142015197.png)



**博弈树搜索**

- 研究博弈树搜索是哪两个方面

  ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302142020586.png)

- 极大极小(基本思想，评估函数值，缺点)

  1. 基本思想

     1. 当轮到MIN走步的节点时，MAX应考虑最坏的情况(即f(p)取极小值);

     2. 当轮到MAX走步的节点时，MAX应考虑最好的情况(即f(p)取极大值);

     3. 评论往回倒推时，相应于两位棋手的对抗策略，交替使用1和2两种方法传递倒推值。

  2. 搜索图

     ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302142023787.png)

  3. 评估函数

     估价函数定义如下:设棋局为P，估价函数为e(P)。

     (1) 若P对任何一方来说都不是获胜的位置，则e(P)=e(MAX)-e(MIN) e(MAX)=所有空格都放上MAX的棋子之后三子成一线的总数 e(MIN)=所有空格都放上MIN的棋子之后三子成一线的总数

     (2) 若P是MAX必胜的棋局，则e(P)=+∞。

     (3) 若P是MIN必胜的棋局，则e(P)=-∞。

  4. 极大极小过程

     ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302142030496.jpeg)

  5. 缺点（ppt没有）

     ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302142036880.png)

- $\alpha-\beta剪枝$（基本过程，缺点）

  - $\alpha$剪枝：极小≤极大
  - $\beta$剪枝：极大≥极小

  ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302142040680.jpeg)

  ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302142041554.jpeg)

- 缺点

  严重依赖节点的寻找顺序

  - 如果你总是先去搜索最坏的节点，那么Beta截断就不会发 生，因此该算法就如同最小-最大一样，效率非常低。该 算法最终会找遍整个博弈树，就像最小-最大算法一样。

  - 如果程序总是能挑最好的着法来首先搜索，那么数学上有 效分枝因子就接近于实际分枝因子的平方根。这是Alpha- Beta算法可能达到的最好的情况。

    这是很大的改进，在搜索结点数一样的情况下，可以使你的搜索深度达到原来的两倍。

## 第四章 不确定性推理

**可信度方法**

- ![传递公式](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302152116888.png)

- ![结论不确定性的合成算法](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302142145807.png)

- ![例题一](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302142110076.png)

- ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302142123158.jpeg)

- ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302142147186.png)

- ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302142150999.jpeg)

  ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302142150028.jpeg)

**主观贝叶斯**

**公式**

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302142225968.jpeg)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302142217592.jpeg)

- EH公式和CP公式不用记，用到会题目会给出

- 难度不会超过综合练习题-2

- ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302142210627.png)

- ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302142234919.jpeg)

  ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302142234948.jpeg)

  ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302142234977.jpeg)

  ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302142236295.jpeg)

  ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302142236331.jpeg)

## 第五章 机器学习

### 5-1

- ![机器学习的一般思路](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302141647704.png)

- 回归问题和分类问题的值域范围

  回归问题：值域为整个实数域;分类问题常为有限的离散点

- 需要知道最小二乘法的局域性，其他的不需要会

  ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302142245627.png)

- 🌟梯度下降（基本步骤，对于最基本的损失函数要知道如何求解，影响因素产生什么样的结果，没有提到BGD不知道要不要考？？）

  1. 梯度下降的基本步骤

     ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302142248399.png)

     ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302142248992.png)

  2. 步长和初始值对梯度下降法的影响

     1. 步长太大，会导致迭代过快，甚至有可能错过最优解 。步长太小，迭代速度太慢，很长时间算法都不能结束 。

     2. 由于梯度下降法不一定能找到全局最优解常为一个局部最优解，所以初始值的选择可能造成我们最后得到的结果不同，处于不同的局部最优解当中。

     3. 算法的不同对结果的影响也是不同的。

        ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302150950169.JPG)

        ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302150950903.JPG)

  3. 梯度下降法的基本例题

  

- 逻辑回归和线性回归的区别

  逻辑回归本质上是基于线性回归演变而来的，一半的回归问题如果采用的是线性回归 + 阈值的方法的就会丢失一般性，所以逻辑回归在线性回归层面上多加一层函数映射 $g(z)$,即将特征进行线性求和，然后使用函数 $g(z)$作为假设函数来进行预测，是y的取值在区间(0,1),这个函数称为Logistic函数，也称为Sigmoid函数.

  对比

  ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302150939625.png)

### 5-3线性分类器

- 线性分类器的三个基本要素![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302150941179.png)

  

- Fisher线性判别（基本思想）

  1. 出发点：把所有的样本都投影到低维空间，使得在低维空间易于分类
  2. 目标:使两类样本投影的均值之差尽可能大些， 而使类内样本的离散程度尽可能小。
  3. 两步分类器设计: 1)确定最优投影方向 2)在这个方向上确定分类阈值
  4. 🌟基本思想：给定训练样本集，把所有样本都投影到一条直线上，使得同类样本的投影点尽可能近，异类样本尽可能远。把新样本都投影到当前的直线上，根据投影点的位置来确定新样本的类别

- 多类问题（有一对多等这些方式，要掌握每一种方式是如何做的，问题是什么；

  1. 一对多
     1. ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302151005331.png)
     2. ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302151005226.png)
  2. 成对可分
     1. ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302151007236.png)
     2. ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302151007272.png)
     3. 结论:判别区间增大，不确定区间减小，比第一种情况小的多.
  3. 最大值判决
     1. 每类都有一个判别函数,存在*M*个判别函数。要判别模式*X*属于那一类，先把**X**代入**M**个判别函数中，判别 函数最大的那个类别就是**X**所属类别。
     2. 结论:不确定区间没有了，所以这种是最好情况，求解计算复杂，收敛较慢

- 🌟感知器的单样本修正法，要会写出基本步骤，例题（多类的不要求）

  1. ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302151047347.jpeg)

  2. ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302151049985.jpeg)

     ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302151049294.jpeg)


### 5-4非线性分类器

**SVM**

1. 什么是最大间隔

   使得训练集D对于线性函数$(w*x)+b$ 的**几何间隔**取最大值的参数对 (w,b)，并由此构造决策函数。

   最大间隔能获得最大稳定性与区分的准确度，从而得到良好的推广能力。

2. 三个方法软/硬间隔以及核方法和主要是用于哪里的

   1. 硬间隔

      ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302151107867.png)

      要求是将每一个样本点都完全分对，在有噪声的情况下强行将样本完全分队可能会造成判别平面的失衡。

   2. 软间隔

      ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302151114490.png)

      简答来说就是在样本点非线性可分的情况下，使用线性分类器会存在一些离群点，软间隔就不要求将离群点完全划分正确。强行划分的话就会对最后划分平面影响很大。（也是与硬间隔最大的区别）

      - 软间隔**SVM**允许样本有一定程度的误分类。但其实质依然是线性分类器。

   3. 核方法

      1. ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302151117654.png)
      2.  核函数只是用来计算映射到高维空间之后的内积的一种简便方法

3. 支持向量是什么

   - 简单来说，就是支持或支撑平面上把两类类别划分开的向量点

   - 支持向量:称训练集**D**中的样本xi为支持向量，如果它对应的 $a_i^* > 0$

   - 正好处于间隔边界上的样本点

   - 例子

     ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302151123054.png)

     ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302151124112.png)

     

4. 软间隔和硬间隔的区别

   1. 软间隔**SVM**允许样本有一定程度的误分类
   2. 硬间隔不允许样本分类错误

5. 核方法的作用是什么

   1. ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302151127087.JPG)

6. 里面的例子给出来 $\alpha$要知道支持向量是如何求解，判别函数如何写

**adaboost**

- 🌟强分类器的设计思想、基本思路

  强分类器设计基本思想:

  1. 选择对正负样本区分最好的几个矩形特征，并训练成对应的弱分类器，把所有 弱分类器组合成高性能的强分类器。
  2. 在训练每个弱分类器时，对正负样本给予不同的权重。在每一轮分类器的训练 过程中，针对上一轮中没有被正确分类的样本，下一轮的分类器就会更加“关注” 它们。每一轮分类器的循环训练是靠样本权值的更新来实现。权值改变的作用是在 被误分的样本上设置更大的权值，在分类正确的样本上减小其权值，凸显分类错误 的样本，使其得到重视。(错的改变分布概率)
  3.  通过训练的 T 个弱分类器的加权投票建立最终的强分类器。

- 级联分类器的设计思想

  adaboost 是多个弱分类器 级联是多个强分类器 级联强分类器的策略是，将若干个强分类器由简单到复杂排列，希望经过训练使每个强分器都有较高检测率而误识率可以放低

  联合多个强分类器，对非人脸采取“先重后轻”，快速、高效的检测出人脸。要求各 级强分类器从简单到复杂，从而有效、快速的排除最多的非人脸

  先重后轻:极联分类器的前几级构造小型、高效的强分类器用来滤除大量负样本， 使待检测的窗口越来越少，再调用后面相对复杂的强分类器，达到较低的误检率

- 为了解决什么:

  1.Adaboost 算法存在退化现象，随着弱分类器个数的增加，强分类器的分类能力反而会降低，需要改进

  2.Adaboost 在检测图像过程中要对每一个位置的每一种尺寸的窗口进行检测，检测量极大

  3.在实际图片中，人脸区域只占整张图片的较小比例，Adaboost 在检测时，会花大量的时间在这些无人脸区域上

## 第六章 人工神经网络

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302141710201.png)

- BP算法的基本思想

  信号的正向传播+误差的反向传播

  - 正向传播时,输入样本从输入层传入,经各隐层逐层处理后, 传向输出层。若输出层的实际输出与期望的输出不符,则转 入误差的反向传播阶段。

  - 反向传播时，将输出以某种形式通过隐层向输入层逐层反传,并将误差分摊给各层的所有单元,从而获得各层单元的误差信号,此误差信号即作为修正各单元权值的依据。 采用梯度下降算法通过调整各层权值求目标函数最小化。
- 径向基函数（只需要了解下图）
- ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302141714629.png)
- RBF与BP神经网络的区别

  ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302151421021.png)

  ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302151426143.png)

## 第七章 深度学习

### 7-1卷积神经网络

- BP算法为什么不能用于深度学习里面

  ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302151427122.png)

- 卷积神经网络的核心思想（局部感知，权值共享，多卷积核，空间下采样）

  1. 局部感知:对局部进行感知，在更高层将局部的信息综合起来就得到了全局信息 
  2. 权值共享:每个神经元的权值设置相同 
  3. 多卷积核:设置多个卷积核，提取多个特征 
  4. 空间下采样:利用图像局部相关的原理，对图像进行子抽样，可以减少数据量的同时保持有效信息，对不同位置的特征进行聚合统计，即池化

- 卷积神经网络的神经元个数、参数（权值）个数 LeNet-5

### 7-2 CNN

- Relu函数主要的优缺点

  优点：![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302151502437.png)

  缺点：

  ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302151502402.png)
- 为什么要正则化，正则化的作用

  - 对于数据集数量有限的情况下，通过降低模型的复杂度防止过拟合
  - 另一种方式就是在cost函数中加入正则化项，正则化项可以理解为复杂度，cost越小越好，但cost加上正则项之后，为了使cost小，就不能让正则项变大，也就是不能让模型更复杂，这样就降低了模型复杂度，也就降低了过拟合。这就是正则化。正则化也有很多种，常见为两种L2和L1。
- Dropout基本思路、目标、与正则化的区别

  - 基本思路

    ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302151508346.png)
  - 目标：减少过拟合
  - 与正则化区别

    ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302151510983.png)

- 批规范化的优化策略

  BP算法反向传播式中有w的存在，所以w的大小影响了梯度变化，造成梯度消失或爆炸，减缓网络收敛速度。因此我们可以通过固定每一层网络输入值的分布来对减缓问题。

  通过让每一层的输出规范为均值和方差一致的方法，消除w带来的放大缩小的影响，进而解决梯度消失和爆炸的问题。


### 7-4集成学习

- ![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302141723941.png)

# 2019-2020年真题



## 简答题

1. 全局择优。。

   1. 把初始点放入open当中，并且使用估计函数计算初始点$f(s_0)$
   2. 查看open表，如果open表为空的话，则说明无解，退出。
   3. 从open表中取出来第一个节点n，并放入close表当中
   4. 查看取出来的的节点n是否是我们目标状态，如果是则寻找到答案，退出
   5. 节点n是否可以拓展，不可以拓展就返回到第二步
   6. 拓展节点n，计算节点n的所有子节点的估价函数，并且放入到open表当中。然后对open的所有节点按照估价函数的升序进行排列。
   7. 返回到第二步

2. SVM

   1. 最大间隔准则

      使得训练集D对于线性函数$w*x + b$的几何间隔最大值的参数对 $(w,b)$,并由此构造决策函数。

      最大间隔可以获得最大稳定性与区分的准确度，从而得到良好的推广能力

   2. 核函数

      不会

3. 级联分类器

   1. adaboost是由若干个弱分类器组成的机器学习模型，那么对于级联分类器来说就是使用若干个强分类器的从简单到复杂进行排列，希望经过训练使得每一个强分类器都有比较高的检测率而误识率尽量放低的学习模型。
   2. 联合多个强分类器，对非人脸进行“先重后轻”，快速，高效的检测出人脸，要求各级联分离器从简单到复杂，从而有效、快速的排除最多的非人脸
   3. 先重后轻：级联分类器的前几级构造小型、高效的强分类器用来滤过大量的负样本、使得待检测的窗口越来越少，再调用后面相对复杂的强分类器这样就可以达到比较低的误检率

4. ReLu函数

二、逻辑回归与线性回归

1.

线性回归：解决的输入特征向量到的输出结果之间的线性关系的预测。 $Y=x\theta$

逻辑回归：解决的一般性的二分类问题

2.![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302151953337.jpeg)

3. ？ 
4.  
   1. 步长（学习率）。步长如果太大的话就会每次更新的太快，可能会错过局部最优解。步长太小的话，更新迭代的速度太慢。
   2. 初始点的选定。因为梯度下降法是一个局部最优的方法，往往求出来的值并不是全局最优而是局部最优。所以每次初始点的选取有很大的可能会影响的我们最后的结果的好坏。
   3. 迭代算法的不同对收敛结果可能会有影响。
      1. BGD，使用全部样本计算损失函数进行对 $\theta$ 的更新。这样的直接导致了算法的收敛速度很慢，但是精确度很好。
      2. SGD 与 MBGD，这两个算法的缺点基本相近。都是因为采用了比较少的样本点进行权重的更新，计算量比较小，训练速度很快。对于精确度来说由于采用了比较少的样本点所以精确度不会很高，可以得到局部最优解。对于收敛速度来说，由于迭代方向的变化很大，不能会快的收敛到局部最优解当中。MBGD的改进，可以在使用MBGD之前将数据随机打乱，然后使用mini-batch进行划分。

三、博弈树

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302152025081.jpeg)

3）严重依赖于搜索顺序

1. 在最坏的情况下，每次搜索的顺序都是最坏的节点，无法进行 $\beta$ 截断。这样最后就会退化成min-max算法，如果在节点数目比较多的情况下，这样的搜索效率是其低，最终搜索会遍布整个博弈树。
2. 如果程序每次都挑中最好顺序的节点来说，在数学上分枝因子就节点实际分枝因子的平方根。也就是说在搜索节点树木相同情况下，我们可以比正常不剪枝的博弈树层数*2（一倍）

四、CNN

1. ？

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302152039925.jpeg)

五、感知器

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302152050875.jpeg)

![](https://124newblog-1309411887.cos.ap-nanjing.myqcloud.com/images/202302152051148.jpeg)
